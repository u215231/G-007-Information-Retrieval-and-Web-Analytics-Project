# Project Part 1 Report
### <p style="color:gray; margin-top:-20px;">Information Retrieval And Web Analysis 2025</p>
---

Marc Bosch Manzano u215231
Chris Matienzo Chilo u198726
Ã€lex Roger Moya u199765

<p style="text-align: justify;">

## GitHub repository link

The GitHub repository with the code of the labs is the following: https://github.com/u215231/Information-Retrieval-and-Web-Analytics-Project.git. Here, you have to navigate to the folder where it is the Part 1 solution: `IRWA-2025-part-1`. The Python notebook with the solution's code is called `IRWA-2025-part-2-final-solution.ipynb`. 

## Introductory Section

For this Part 2 of the project, we have implemented several functions and classes in order to solve all the questions. We have structured them into some python source code files described below.

- **`processing.py`** It contains functions to process the lines of text. In particular, it contains the *build_terms* function and the definitions of stemmer object and stop words. It is used in section 1.1.<br>

- **`index.py`**: It contains the class called *Indexing* that controls the data structure of the index. It is used in section 1.1.<br>

- **`ranking.py`**: It contains the *Ranking* class that is used to create the ranking of documents given the queries. There are quite a few functions, including those that create the term frequency, the inverse document frequency, the weights for documents or queries, etc. The most relevant function is *rank_tfidf_dataframe*, which returns a dataframe with the queries, the top K documents with the best scores, and the scores for each document and query. It is used in sections 1.3 and following.<br>

- **`evaluation.py`**: It contains the *Evaluation* class to perform the analysis of the documents predicted as relevant for the ranking. Th class constructor recieves a *search_results* dataframe, which is a dataframe containing a *query_id* (a query category, which in our case is 1 or 2 for validations.csv), some labels of the documents (true or false) ,and the scores of the documents. Here, there are all the functions that were requested for sections 2.1, 2.2 and 2.3: precision at K, recall at K, average precision at K, etc.

All the explanations and the documentation of the classes and functions can be found on the respective files, since we have used python docstrings to comment the codes.


## Part 1: Indexing 
 
<b>1.1.  Build inverted index: After having pre-processed the data, you can then create the inverted index.</b>

Previously, we have generated the processed version of the Fashion Products Dataset as CSV file. We have added a code to exported it at the bottom part of project 1 solution notebook (see: [IRWA-2025-part-1-final-solution.ipynb](../IRWA-2025-part-1/IRWA-2025-part-1-final-solution.ipynb)).

Then, we have imported on the processed dataframe as `df_fashion_products`. We have focused on columns `pid` and `document`, which contain the document identifiers and document texts. The column *document* is defined on project part 1 notebook, which contains all the contents of the title, description, and categorical variables merged.

Here, we have applied the `build_inverted_index` function to these documents. We have tried to send an example query to the index, "*women cotton dress"*, using `search_by_conjunctive_queries` queries. It has retrieved all the documents having the words of the query.
 
<b>1.2.  Propose test queries: Define five queries that will be used to evaluate your search 
engine.</b>

Here we have proposed five queries and we have arranged them as a Pandas series, identifying each query by an integer.
 
<b>1.3.  Rank your results: Implement the TF-IDF algorithm and provide ranking-based results.</b>

**Theoretical TF-IDF Model**

Here is the mathematical formulation of the TF-IDF model we have used.

Let the following definitions of our inputs:
- $V = {k_1, k_2, ..., k_t}$ is the vocabulary, the set of terms, where $t$ is the number of terms in the vocabulary.<br>
- $d_1, d_2, ..., d_N$ is the documents of the collection, where $N$ is the number of documents in the collection.<br>
- $i \in \{1, 2, ..., t\}$ the index for a term in the vocabulary.<br>
- $j \in \{1, 2, ..., N\}$ the index for a document in the collection.<br>
- $q$ is a query.<br>
  
Let the following definitions of the metrics of the vector model:<br>
- $f_{i,j}$ be the frequency of occurrence of term $i$ in document $j$.<br>

- $df_i$ be the document frequency for term $i$, that is, the number of documents where term $i$ occurs, defined as $df_i =  |\{j:f_{i,j} > 0\}|$.<br>

- $tf_{i,j}$ be the term frequency for term $i$ in document $j$, defined as $$tf_{i,j} = 1 + log_2(f_{i,j})$$<br>

- $idf_i$ be the inverse document frequency defined for term $i$, defined as $$idf_i = log\dfrac{N}{df_i}$$<br>

- Let $w_{i,j}$ be the term weights or TF-IDF weights for term $i$ and document $j$, defined as $$w_{i,j} = \begin{cases} tf_{i,j} \cdot idf_{i} & \text{if } f_{i,j} > 0\\ 0 & \text{otherwise} \end{cases}$$<br>

Similarly, we define the frequency $f_{i,q}$, term frequency $t_{i,q}$, and weights $w_{i,q}$ of term $i$ for query $q$. <br>

Let $\overrightarrow{d_j} = [w_{1,j}, w_{2,j}, ..., w_{t,j}]^T$ be the weighted vector for document $j$<br> Let $\overrightarrow{q} = [w_{1,q}, w_{1,q}, ..., w_{t,q}]^T$ be the weighted vector for query $q$.

We define the similarity between document $j$ and query $q$ as the cosine for the angle between vectors $\overrightarrow{d_j}$ and $\overrightarrow{q}$: $$\text{sim}(\overrightarrow{d_j}, \overrightarrow{q}) = \dfrac{\langle \overrightarrow{d_j}, \overrightarrow{q} \rangle}{||\overrightarrow{d_j}|| \cdot ||\overrightarrow{q}||}$$ where $\langle \cdot, \cdot\rangle$ is the inner product of two vectors and $||\cdot||$ is the Euclidean norm of a vector, both in space $\mathbb{R}^t$. This formula is also called the cosine similarity between vectors.

We define our ranking for our query $q$ as the first top $K$ documents retrieved by ordering our similarities in a descending way. If our sorted similarities are $$\text{sim}(\overrightarrow{d}_{(1)}, \overrightarrow{q}) \geq \text{sim}(\overrightarrow{d}_{(2)}, \overrightarrow{q}) \geq ... \geq \text{sim}(\overrightarrow{d}_{(K)}, \overrightarrow{q}) \geq ... \geq \text{sim}(\overrightarrow{d}_{(N)}, \overrightarrow{q})$$
Therefore, our retrieved documents are those in the $K \leq N$ ordered greater scores, that is, the tuple of document indices $(j : j = (r))_{r=1}^K$, where $(\cdot)$ is a function that assings a rank number $r \in {1, ..., N}$ to the real document index number $j \in {1, ..., N}$.

**Implemented TF-IDF Model**

Our implementation has consisted of a class called *Ranking*. It uses dictionaries in order to optimize the memory consumption and make more efficient the computations. 

The class on its constructor recieves two series: a documents series, with identifier and document text content, and a queries series, with identifier and query text content, too. This makes the ranking process more generalised: it can manage several queries with the same collection of documents. We have furthermore two setters, *set_documents* and *set_queries*, wether the user wants to change the collection of the ranking object.

The framework is managed by several methods and atributes document. Each method computes the some metrics (frequencies, weights, etc.), and it is able to return it as well as modify the value of the corresponding attribute. For example, the *Ranking.get_df* function returns a dictionary of document frequencies of the terms, but sets the value to the attribute *self.df*, which stores the document frequencies. Each of these methods, when it has already computed one of these metrics and both the documents and the queries have not changed, directly returns the value of the corresponding attribute. This optimizes the execution time of document ranking calculations. In order to recalculate metrics, new documents or queries must be entered into the setters, otherwise they will not be recalculated.

The attributes are mainly dictionaries or compositions of dictionaries, as we have said, in order to save memory and improve the speed of calculation. For example, in the case of occurrence frequencies *f*, if they are computed for all the documents in the collection, they form a dictionary with the document identifier as key and, as value, another dictionary with the term and its frequency as another.

The calculation of the similarity scores for documents as queries is done in *rank_tfidf_dict*. Here, we retrieve a dictionary of the top K scored documents for each query. It has as keys the query identifier, and as values a dictionary of documents identifier and each score, respectively. The similarity it is computed by the terms of the query that appear on the document. This helps optimize the calculation of similarities, avoiding having to do the dot products of vectors with as many dimensions as the terms in the vocabulary. So, the formula we have used for the calculations of the scores can be stated as following: $$\text{sim}(d_j, q) = \dfrac{\sum_{i : k_i \in d_j \cap q} w_{i, j} w_{i, q}}{\sqrt{\sum_{i : k_i \in d_j} w_{i, j}^2}\sqrt{\sum_{i : k_i \in q} w_{i, q}^2} }$$

We have not eliminated the normalization of scores by query length, as we we will evaluate the scores in the next sections. We need all scores in the range $[0, 1]$ in orther to facilitate the analisys.

We have also defined *rank_tfidf_dataframe*, which converts the scores to a dataframe of query identifiers, document identifiers, and scores. This method is used in the evaluation section to compute the ranked documents for the *validation_labels.csv* file.

Finally, we have a *print_rankings* method to visualize the ranked document for each query one by one. We have other static methods an auxiliary methods in the class wich are self explained in the [*ranking.py*](./ranking.py) file.

Object methods of *Ranking* class.

```bash
def set_items(items, type]) -> None:
def set_documents(documents) -> None:
def set_queries(queries) -> None:
def get_df() -> dict:
def get_idf() -> dict:
def get_f(type) -> dict:
def get_tf(type) -> dict:
def get_tfidf(type) -> dict:
def rank_tfidf_dict(topK) -> dict:
def rank_tfidf_dataframe(topK) -> pd.DataFrame:
def print_rankings(topK) -> None:
```

Static methods or *Ranking* class.

```bash
def get_frequencies_from_text(text) -> dict:
def get_tf_from_frequency(frequencies) -> dict:
def get_weights_from_tf_idf(tf, idf) -> dict:
def get_cosine_similarity(document_vector, query_vector) -> float:
def get_norm(vector) -> float:
def get_top_scores(document_scores, topK) -> dict:
```

**Results**

We can see the ranked documents we have obtained for our five queries are quite relevant, as they contain many of the words in the queries most of the time.
 
## Part 2: Evaluation 
 
1.  Implement the following evaluation metrics to assess the effectiveness of your retrieval solutions. These metrics will help you measure how well your system retrieves relevant documents for each query: 
    - i.  Precision@K (P@K) 
    - ii.  Recall@K (R@K) 
    - iii.  Average Precision@K (P@K) 
    - iv.  F1-Score@K 
    - v.  Mean Average Precision (MAP) 
    - vi.  Mean Reciprocal Rank (MRR) 
    - vii. Normalized Discounted Cumulative Gain (NDCG) 



2.  Apply the evaluation metrics you have implemented to the search results and relevance judgments provided in validation_labels.csv for the predefined queries. When reporting evaluation results, provide only numeric values, rounded to three decimal places. Do not include textual explanations or additional statistics in this section. 
    - a.  Query 1: women full sleeve sweatshirt cotton 
    - b.  Query 2: men slim jeans blue 
 
3.  You will act as expert judges by establishing the ground truth for each document and 
query.  
    - a.  For the test queries you defined in Part 1, Step 2 during indexing, assign a binary relevance label to each document: 1 if the document is relevant to the query, or 0 if it is not. 
    - b.  Comment on each of the evaluation metrics, stating how they differ, and which information gives each of them. Analyze your results. 
    - c.  Analyze the current search system and identify its main problems or limitations. For each issue you find, propose possible ways to resolve it. Consider aspects such as retrieval accuracy, ranking quality, handling of different field types, query formulation, and indexing strategies.
