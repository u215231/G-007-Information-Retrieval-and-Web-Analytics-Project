# Project Part 2 Report
### <p style="color:gray; margin-top:-20px;">Information Retrieval And Web Analysis 2025</p>
---

Marc Bosch Manzano u215231
Chris Matienzo Chilo u198726
√Älex Roger Moya u199765

<p style="text-align: justify;">

## GitHub repository link
The GitHub repository with the code of the labs is the following: https://github.com/u215231/Information-Retrieval-and-Web-Analytics-Project.git. Here, you have to navigate to the folder where it is the Part 3 solution: `IRWA-2025-part-3`. The Python notebook with the solution's code is called `IRWA-2025-part-3-final-solution.ipynb`. 

## Introductory Section
This section closely follows the introductory section of Part 2.
We have started our code by initializing our dataframe df_fashion_products that contains our Fashion Products Dataset. In all modifications to this dataframe we have adopted the convention to append a suffix _XX, where XX is the modification number.

Furthermore, in this previous section, we have displayed the column names, the null count values, and data types of our fashion products dataset at the beginning of the notebook. We have also displayed the first rows and the number of rows and columns of the dataset to see in order to see how the dataset is structured.


## Part 1: Ranking
<b>1.  You‚Äôre asked to provide 3 different ways of ranking: 

a.  TF-IDF + cosine similarity: Classical scoring, which we have also seen during the 
practical labs 

b.  BM25  

c.  Your Score: Here, the task is to create a new score. (Be creative üé®, think about 
what factors could make a document more relevant to a query and include them 
in your formula.)
 
Explain how the ranking differs when using TF-IDF and BM25, and think about the pros 
and  cons  of  using  each  of  them.  Regarding your own score, justify the choice of the 
score (pros and cons). HINT: Look into numerical fields that each record has to build your 
score.</b>

To accommodate the new ranking functions, we replicated the Ranking class from Part 2 and implemented the additional functionality within this revised version.

### A. TF-IDF + Cosine Similarity
This method represents both the user‚Äôs query and the documents as vectors in a high-dimensional space. Each dimension corresponds to a unique word in the vocabulary.

<b>TF-IDF Weight Definitions</b>
<br><br>
TF-IDF is a standard weighting scheme that balances two key factors:

- Term Frequency (TF): how often a term appears in a document or query ‚Üí captures importance within that text.

- Inverse Document Frequency (IDF): how rare the term is across the entire collection ‚Üí reduces the weight of common, non-informative terms.

Thus, the weights

$$W_{t,d} = TF_{t,d} \times IDF_t \quad and \quad W_{t,q} = TF_{t,q} \times IDF_t$$

ensure that both query and document vectors emphasize meaningful and distinguishing terms.

<br>
<b>Dot product</b>
The dot product is computed by iterating over all terms in the query vector. Any term not present in the document vector is assumed to have weight zero:

$$\sum_{t \in Q} W_{t,q} \cdot W_{t,d}$$

This works because terms not in the query cannot contribute to similarity, iterating over query terms is more efficient than iterating over all vocabulary terms.

This is a common optimization used in search engines where queries are short but documents are large.
<br>
<b>Final Cosine Similarity Score</b>

Cosine Similarity calculates the cosine of the angle between two vectors (Query Vector $Q$ and Document Vector $D$). A score of 1.0 means vectors are identical, while 0.0 means they share no terms.

The final cosine similarity score is computed as:

$$\frac{\sum_{t \in Q} W_{t,q} \cdot W_{t,d}}{\|Q\| \cdot \|D\|}$$

<br>
This ranking method serves as the baseline for the project. It effectively captures keyword matching and weights rare terms heavily.
<br><br>
<b>Functions implemented/used:</b>

- `rank_tfidf_filtered(query_terms, topK)`: It takes a query vector computes the pre-filtered list of documents (from the conjunctive query), retrieves their pre-calculated TF-IDF vectors, calculates the cosine similarity against the query vector, and returns the top K sorted results.

- `get_cosine_similarity(document_vector, query_vector)`: Mathematically computes the cosine similarity between two sparse dictionary vectors using the dot product divided by the product of their Euclidean norms.

### B. BM25 (Best Match 25)
BM25 is a probabilistic ranking function that estimates the likelihood that a document $d$ is relevant to a query $Q$, based on term frequency saturation and document length normalization. 

<b>BM25 Ranking Formula</b>
<br>
Given a query $Q = \{ t_1, \ldots, t_n \}$, the BM25 score for a document $d$ is:
$$Score_{BM25}(Q,d) = \sum_{i=1}^{n} IDF(t_i) \cdot \frac{f(t_i, d) \cdot (k_1 + 1)}{f(t_i, d) + k_1 \left(1 - b + b \cdot \frac{|d|}{avgdl}\right)}$$

Where:
- $f(t_i,d)$ is the term frequency of $t_i$ in document $d$.
- $|d|$ is the length of the document
- $avgdl$ is the average document length across the collection
- $k_1$ controls term saturation (typically $1.2 \le k_1 \le 2.0$). 
- $b$ controls length normalization (typically $b=0.75$). 

<br>
<b>Term saturation</b>

In TF-IDF, every additional occurrence of a term increases the score linearly:
$$TF-IDF contribution‚àùf(t,d)$$
This is unrealistic because encountering a word 10 times is not 10√ó more important than seeing it once.

BM25 solves this using a saturation curve:
$$\frac{f(t_i, d) \cdot (k_1 + 1)}{f(t_i, d) + k_1}$$
As $f(t,d)$ grows, the contribution approaches a finite limit.

<br>
<b>Length normalization</b>
Longer documents naturally contain more terms, which can bias them upward. BM25 applies a penalty when a document is longer than average:

$$1‚àíb+b‚ãÖ\frac{|d|}{avgdl}$$

If $|d| > avgdl$, the denominator increases then score decreases, but if $|d| > avgdl$, the term contribution is boosted.

This makes scoring fair across documents of different lengths.

<b>Functions implemented/used:</b>

- `calculate_bm25_statistics()`: Calculates and stores the length of each document (`doc_lengths`) and the average document length (`avgdl`).

-  `get_score_bm25_term(term, doc_id, k1, b)`: Calculates the BM25 score contribution for a single term $t$ in a single document $d$. In other words, applies the formula:
    $$Score_{BM25}(t,d) = IDF(t) \cdot \frac{f(t, d) \cdot (k_1 + 1)}{f(t, d) + k_1 \left(1 - b + b \cdot \frac{|d|}{avgdl}\right)}$$


- `rank_bm25(query_terms, topK)`: Iterates through the filtered documents, sums the BM25 scores for all query terms, and returns the top $K$ ranked documents.


### C. Custom Score
Traditional information retrieval models like TF-IDF and BM25 focus exclusively on textual relevance, determining how well a document matches the query terms. However, in real-world applications, relevance alone is insufficient. Users prefer items that are not only relevant but also of high quality or popularity.

To address this, we implemented a hybrid ranking model. This model combines a textual score (dependent on the query) with a quality score (independent of the query) to produce a final ranking that reflects both user intent and product quality.

<br>
<b>Mathematical formula</b>

We utilized a linear combination approach, which allows us to weigh the importance of text matching against product quality.
$$FinalScore = \alpha\ ¬∑ RelScore(d,q) + (1-\alpha)\ ¬∑ MetaScore(d)$$
Where:
- $d$ is the document (product).
- $q$ is the user query.
- $\alpha$ is a tuning parameter ($0\le \alpha \le 1$) that determines the weight of textual relevance. We selected $\alpha = 0.7$, meaning the final score is derived 70% from text relevance and 30% from product quality.

<b>Component 1</b>: $RelScore$ (Textual Relevance):
<br>
For the textual component, we utilized TF-IDF + Cosine Similarity.
We decided to use this method instead of BM25 because BM25 is unbounded, which means that its scores can range between 0 and 20, for example. The effect of adding a small quality signal (0 to 1) to a large BM25 score would be negligible. 

To keep the score between 0 and 1 we used cosine similarity, since it is naturally bounded between 0.0 and 1.0. This normalized range makes it mathematically compatible for combination with other normalized signals.

<b>Component 2</b>: $MetaScore$ (Quality Signal):
<br>
We selected the `average_rating` field as our proxy for product quality. This field represents explicit user feedback.

Raw ratings typically range from 1 to 5 (or 0 to 5). To combine this with the Cosine Similarity (0-1), we applied Min-Max Normalization to scale the ratings to a strict [0, 1] range:

$$MetaScore(d) = \frac{Rating(d) - Rating_{min}}{Rating_{max} - Rating_{min}}$$

<br>
A significant challenge in hybrid ranking is missing metadata (new products with no ratings).
As a solution, we performed a data cleaning step to convert non-numeric ratings to NaN and then filled them with 0.0.
This ensures that unrated products do not break the ranking pipeline, though it places them at a disadvantage compared to highly-rated items.

<br>
<b>Functions implemented:</b>

- `rank_custom_score(query_terms, topK, alpha)`: 

    1. Retrieval: It accepts a query and computes the filtered list of documents containing all query terms (Conjunctive Query).
    2. Relevance Calculation: It computes the TF-IDF Cosine Similarity for each document against the query vector.
    3. Metadata Lookup: It retrieves the `average_rating` from the dataset for each filtered document.
    4. Normalization: It applies the min-max formula using global dataset statistics.
    5. Combination: It applies the linear formula using the defined $\alpha$.
    6. Sorting: Finally, it returns the top K documents based on this new hybrid score.

This ranking method tends to favor "safe bets". For a query like "running shoes", pure TF-IDF might return a specialized, unknown shoe with a perfect keyword match. On the other hand, this custom score, might rank a slightly less textually perfect match higher if that product has a 4.9/5.0 rating with thousands of reviews, which aligns better with typical user purchasing behavior.

<br>
Explain how the ranking differs when using TF-IDF and BM25, and think about the pros and  cons  of  using  each  of  them.  Regarding your own score, justify the choice of the score (pros and cons). HINT: Look into numerical fields that each record has to build your score.   


## Part 2: Word2vec

<b>2.  Implement word2vec + cosine ranking score. Return a top-20 list of documents for each 
of  the  5  queries  defined  in  the  Part  2  of  your  project, using search and word2vec + 
cosine similarity ranking. 
   
To represent a piece of text using word2vec, we create a single vector that represents 
the entire text. This vector has the same number of dimensions as the word vectors and 
is calculated by averaging the vectors of all words in the text. </b>

Unlike TF-IDF or BM25, which rely on exact keyword matching, this method utilizes vector representations to capture semantic meaning. We represent both the query and the documents as vectors in a continuous vector space, where text with similar meanings is located closer together.

<b>Implementation:</b>

We implemented two core functions to handle vectorization and the retrieval pipeline:

- `text_to_vec_w2v(self, text, w2v)`: This helper function transforms a raw text string into a single, normalized vector.

    1. Splits the text and retrieves the pre-trained Word2vec embedding for each word, ignoring words that are not present in the model.
    2. Computes the mean of all valid word vectors, effectively finding the "center of gravity" of the document in the semantic space.
    3. The resulting vector is normalized to unit length ($‚à£‚à£v‚à£‚à£=1$). This optimization allows us to calculate the cosine similarity later using a simple dot product.

- `rank_w2v_and(self, w2v, topK=20)`: This function is in charge of the retrieval and ranking process.

    1. To improve efficiency during query processing, we first pre-calculate and store the normalized vectors for all documents in the collection (`doc_vecs`).
    2. For each query, we first use the Inverted Index (`indexer.search_by_conjunctive_queries`) to retrieve only those documents that contain all query terms. This ensures basic relevance and reduces the search space.
    3. We compute the cosine similarity between the normalized query vector and the normalized document vectors of the candidate set.
    4. The matching documents are sorted by their similarity score in descending order, and the top 20 are returned.

## Part 3: Alternative representations
<b>3.  Can you imagine a better representation than word2vec? Justify your answer. 
(HINT - what about Doc2vec? Sentence2vec? What are the pros and cons?) </b>

Word2vec is a useful method that captures semantic context (e.g., "apparel" matches "clothing") better than keyword matching, but loses text information by averaging word vectors, which can dilute the meaning of long documents. 

A better representation could be Doc2vec. It learns a specific vector for each document and captures the full context. 
Other models like Sentence2vec offer superior results since they take into account the structure and global meaning of the text. 

This alternatives tend to provide a more precise semantic similarity than Word2vec. 

